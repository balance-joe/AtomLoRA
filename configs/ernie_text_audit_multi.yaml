# configs/ernie_text_audit_multi.yaml
# 文本审计模型配置：ERNIE-3.0 + LoRA微调 + 错词标记识别
# 快速训练版本：适用于本地调试和验证

# --- 1. 实验基本定义 ---
exp_id: "ernie_text_audit_multi_task_final"                    # 实验ID：用于日志和模型保存的标识
description: "文本审计任务：误报和风险识别(2分类)。LoRA微调+快速训练配置"  # 实验描述：说明任务和目标
base_config: "templates/ernie_lora_template.yaml"          # 基础配置：继承模板，避免重复配置
task_type: "multi_cls"                      # 任务类型：双任务分类，简化处理逻辑

# --- 2. 数据配置 ---
data:
  # 数据路径配置 - 请根据实际文件位置修改
  train_path: "./data/raw/ernie_text_audit_multi/train.jsonl"  # 训练数据路径：JSONL格式
  dev_path: "./data/raw/ernie_text_audit_multi/dev.jsonl"       # 验证数据路径：用于早停和评估
  test_path: "./data/raw/ernie_text_audit_multi/test.jsonl"      # 测试数据路径：最终模型测试
  max_len: 256                                                     # 文本最大长度：过长截断，不足填充
  
  text_col: "text"                           # 文本字段名：数据中文本内容的列名
  
  # 标签配置 - 双任务误报识别
  label_col: 
    misreport: "is_misreport_label"          # 误报标签列：数据中标签的字段名
    risk: "risk_label"
    
  # 标签取值范围 - 用于自动创建映射关系
  label_subset: 
    misreport: [0, 1]                        # 标签取值：0=非误报，1=误报
    risk: [0, 1, 2, 3, 4]            # 标签2

  # 标签映射关系 - 数字索引到可读文本
  label_mapping:
    misreport: {0 : "非误报", 1 : "误报"}    # 标签解释：索引到中文的映射
    risk: {0 : "一级", 1 : "二级", 2 : "三级", 3 : "四级", 4 : "五级"}


# --- 3. Tokenizer配置 - 错词标记特殊处理 ---
tokenizer:
  add_special_tokens: True                   # 添加特殊标记：启用自定义token扩展
  special_tokens: 
    - "[ERROR]"                              # 错误开始标记：标识错词起始位置
    - "[/ERROR]"                             # 错误结束标记：标识错词结束位置
    - "NOSUGGEST"                            # 无建议标记：标识无需修正的词
    
# --- 4. 模型架构配置 ---
model:
  dropout: 0.15                              # 丢弃率：防止过拟合，增加模型鲁棒性
  freeze_bert: True                          # 冻结BERT：只训练LoRA和分类头
  arch: "nghuyong/ernie-3.0-base-zh"         # 模型架构：使用ERNIE-3.0中文模型
  path: "./models/ernie-3.0-base-zh"         # 模型路径：从HuggingFace加载
  
  # LoRA 配置 - 快速训练版本
  lora:
    rank: 2                                  # 秩大小：低秩矩阵维度，越小训练越快
    alpha: 4                                 # 缩放系数：控制LoRA更新幅度，通常为2倍秩
    dropout: 0.01                            # 丢弃率：LoRA层丢弃概率，防止过拟合
    target_modules: ["self.query", "self.key", "self.value"] # 目标模块：仅适配query和key，减少参数量
    bias: "none"                             # 偏置项：不训练偏置，节省显存和加速

# --- 5. 训练超参数配置 - 快速训练优化 ---
train:
  num_epochs: 2                              # 训练轮数：快速验证，2轮看趋势
  batch_size: 8                              # 批次大小：根据显存调整，小批次省内存
  gradient_accumulation_steps: 1             # 梯度累积：1=不累积，加快参数更新
  warmup_ratio: 0.05                         # 热身比例：5%步数热身，稳定训练初期
  
  # 早停策略 - 防止过拟合
  early_stopping:
    patience: 1                              # 耐心值：1轮不提升就停止，快速训练
    metric: "avg_f1"                         # 监控指标：双任务使用avg_F1分数作为早停依据
    
  # 损失权重 - 多任务时调整各任务重要性
  loss_weight: [1.0 , 1.1]                   # 权重配置：双任务需两个权重值

  # 优化器配置 - 分组学习率
  optimizer:
    type: "AdamW"                            # 优化器类型：AdamW，带权重衰减
    # 分组学习率：不同参数组使用不同学习率
    groups:
      bert: 2e-6                             # BERT参数：极低学习率，微调模式
      lora: 2e-4                             # LoRA参数：较高学习率，快速适配
      classifier: 8e-4                       # 分类头：高学习率，快速收敛
  
  # 学习率调度器
  scheduler_type: "cosine"                   # 调度类型：余弦退火，平滑下降

# --- 6. 资源调度配置 ---
resources:
  priority: "high"                           # 任务优先级：高优先级资源分配
  gpus: "auto"                               # GPU设置：自动检测可用GPU