# src/predict/predictor.py
import os
import torch
import json
import yaml
from typing import Dict, List, Any, Union, Optional
from tqdm import tqdm
import numpy as np

from src.utils.logger import get_logger
from src.model.model_factory import TaskTextClassifier, load_tokenizer

# ä¸è®­ç»ƒä»£ç å¯¹é½çš„é»˜è®¤å‚æ•°ï¼ˆç»Ÿä¸€ç®¡ç†ï¼‰
DEFAULT_CONFIG = {
    "data": {
        "max_len": 256,
        "text_col": "text"
    },
    "model": {
        "arch": "bert-base-chinese",
        "fp16": False
    },
    "train": {
        "batch_size": 20
    },
    "task_type": "single_cls"
}


class TextAuditPredictor:
    """
    æ–‡æœ¬å®¡è®¡é¢„æµ‹å™¨ - å…¼å®¹å•/å¤šä»»åŠ¡ï¼Œä¸è®­ç»ƒä»£ç æ·±åº¦å¯¹é½
    """
    
    def __init__(self, config: Dict[str, Any], device: Union[str, torch.device, None] = None):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸ï¼ˆä¸è®­ç»ƒé…ç½®ä¸€è‡´ï¼‰
            device: é¢„æµ‹è®¾å¤‡ (None=è‡ªåŠ¨é€‰æ‹©, "cuda:0"/"cpu"=æ‰‹åŠ¨æŒ‡å®š)
        """
        # åˆå¹¶é…ç½®ä¸é»˜è®¤å€¼ï¼ˆå…œåº•ï¼‰
        self.config = self._merge_config(config)
        self.exp_id = self.config['exp_id']
        
        # çµæ´»çš„è®¾å¤‡é…ç½®
        self.device = self._setup_device(device)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = get_logger(f"predict_{self.exp_id}")
        
        # å®éªŒäº§ç‰©ç›®å½•
        self.exp_dir = os.path.join("./outputs", self.exp_id)
        if not os.path.exists(self.exp_dir):
            raise FileNotFoundError(f"å®éªŒç›®å½•ä¸å­˜åœ¨: {self.exp_dir}")
        
        # åŠ è½½å®éªŒäº§ç‰©å’Œæ¨¡å‹
        self._load_experiment_artifacts()
        self._initialize_model()
        
        self.logger.info(f"âœ… é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ | è®¾å¤‡: {self.device} | ä»»åŠ¡ç±»å‹: {self.task_type}")
    
    def _merge_config(self, user_config: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’åˆå¹¶ç”¨æˆ·é…ç½®ä¸é»˜è®¤é…ç½®"""
        def recursive_merge(default: Dict, user: Dict) -> Dict:
            merged = default.copy()
            for k, v in user.items():
                if isinstance(v, dict) and k in merged and isinstance(merged[k], dict):
                    merged[k] = recursive_merge(merged[k], v)
                else:
                    merged[k] = v
            return merged
        
        return recursive_merge(DEFAULT_CONFIG, user_config)
    
    def _setup_device(self, device: Union[str, torch.device, None]) -> torch.device:
        """çµæ´»è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device is None:
            return torch.device("cuda" if torch.cuda.is_available() else "cpu")
        elif isinstance(device, str):
            return torch.device(device)
        elif isinstance(device, torch.device):
            return device
        else:
            raise ValueError(f"æ— æ•ˆçš„è®¾å¤‡ç±»å‹: {type(device)}")
    
    def _validate_config(self):
        """éªŒè¯å…³é”®é…ç½®é¡¹"""
        required_keys = [
            ("exp_id", str),
            ("model", dict),
            ("data", dict)
        ]
        
        for key, expected_type in required_keys:
            if key not in self.config or not isinstance(self.config[key], expected_type):
                raise ValueError(f"é…ç½®ç¼ºå¤±æˆ–ç±»å‹é”™è¯¯: {key} (æœŸæœ›{expected_type.__name__})")
    
    def _load_experiment_artifacts(self):
        """åŠ è½½å®éªŒäº§ç‰©ï¼ˆå…¼å®¹è®­ç»ƒä»£ç çš„ä¿å­˜æ ¼å¼ï¼‰"""
        try:
            # éªŒè¯é…ç½®
            self._validate_config()
            
            # 1. ç¡®å®šä»»åŠ¡ç±»å‹
            self.task_type = self.config.get("task_type", "single_cls")
            
            # 2. å¤„ç†æ ‡ç­¾æ˜ å°„ï¼ˆå¯¹é½è®­ç»ƒä»£ç çš„ä»»åŠ¡åï¼‰
            self.label_map = self.config["data"]["label_mapping"]
            if self.task_type == "single_cls":
                # å•ä»»åŠ¡ä¸‹ï¼šdefaultå¯¹åº”ç¬¬ä¸€ä¸ªæ ‡ç­¾æ˜ å°„
                first_task_key = next(iter(self.label_map.keys()))
                self.label_map = {"default": self.label_map[first_task_key]}
            
            # 3. åŠ è½½tokenizer
            self.tokenizer = self._load_tokenizer()
            
            self.logger.info(f"ğŸ“ å®éªŒäº§ç‰©åŠ è½½æˆåŠŸ | ä»»åŠ¡ç±»å‹: {self.task_type}")
            
        except Exception as e:
            self.logger.error(f"âŒ å®éªŒäº§ç‰©åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_tokenizer(self):
        """åŠ è½½Tokenizerï¼ˆä¼˜å…ˆåŠ è½½è®­ç»ƒå¥½çš„ï¼Œå›é€€åˆ°é…ç½®ï¼‰"""
        tokenizer_path = os.path.join(self.exp_dir, "tokenizer")
        if os.path.exists(tokenizer_path):
            from transformers import AutoTokenizer
            return AutoTokenizer.from_pretrained(tokenizer_path)
        else:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„Tokenizerï¼Œä»é…ç½®åŠ è½½")
            return load_tokenizer(self.config)
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆåŒ¹é…è®­ç»ƒä»£ç çš„ç»“æ„ï¼‰"""
        try:
            # åˆå§‹åŒ–æ¨¡å‹ç»“æ„
            self.model = TaskTextClassifier(self.config, self.tokenizer)
            self.model.to(self.device)
            self.model.eval()
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self._load_model_weights()
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _load_model_weights(self):
        """åŠ è½½æ¨¡å‹æƒé‡ï¼ˆå…¼å®¹è®­ç»ƒä»£ç çš„ä¿å­˜é€»è¾‘ï¼‰"""
        # 1. åŠ è½½LoRAé€‚é…å™¨
        lora_path = os.path.join(self.exp_dir, "lora_adapter")
        if os.path.exists(lora_path):
            try:
                from peft import PeftModel
                dtype = torch.float16 if self.config["model"].get("fp16", False) else torch.float32
                self.model.bert = PeftModel.from_pretrained(
                    self.model.bert, 
                    lora_path,
                    torch_dtype=dtype
                )
                self.logger.info("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                self.logger.error(f"âš ï¸ LoRAåŠ è½½å¤±è´¥: {e}")
        
        # 2. åŠ è½½åˆ†ç±»å¤´æƒé‡
        classifier_path = os.path.join(self.exp_dir, "classifiers.pt")
        if os.path.exists(classifier_path):
            try:
                classifier_state = torch.load(classifier_path, map_location=self.device)
                self.model.classifiers.load_state_dict(classifier_state)
                self.logger.info("âœ… åˆ†ç±»å¤´æƒé‡åŠ è½½æˆåŠŸ")
            except Exception as e:
                self.logger.error(f"âš ï¸ åˆ†ç±»å¤´åŠ è½½å¤±è´¥: {e}")
        
        # 3. å›é€€ï¼šåŠ è½½å®Œæ•´æ¨¡å‹
        model_path = os.path.join(self.exp_dir, "model.pt")
        if os.path.exists(model_path) and not hasattr(self.model.bert, "peft_config"):
            try:
                state_dict = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(state_dict)
                self.logger.info("âœ… å®Œæ•´æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
            except Exception as e:
                self.logger.error(f"âš ï¸ å®Œæ•´æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    
    def predict(self, data_sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        é¢„æµ‹å•æ¡æ•°æ®
        
        Args:
            data_sample: åŒ…å«textå­—æ®µçš„å­—å…¸
            
        Returns:
            Dict: é¢„æµ‹ç»“æœï¼ˆå«æ ‡ç­¾ã€ç½®ä¿¡åº¦ç­‰ï¼‰
        """
        # è¾“å…¥éªŒè¯
        text_col = self.config["data"]["text_col"]
        if text_col not in data_sample:
            raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {text_col}")
        
        text = data_sample[text_col]
        
        # Tokenize
        encoded = self.tokenizer.encode_plus(
            text,
            padding='max_length',
            truncation=True,
            max_length=self.config["data"]["max_len"],
            return_tensors='pt',
            return_attention_mask=True
        )
        
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(input_ids, attention_mask, labels=None)
        
        # åå¤„ç†ç»“æœ
        return self._postprocess_result(outputs, data_sample)
    
    def predict_batch(self, data_samples: List[Dict[str, Any]], 
                     batch_size: int = None, show_progress: bool = True) -> List[Dict[str, Any]]:
        """
        é«˜æ•ˆæ‰¹é‡é¢„æµ‹ï¼ˆGPUå¹¶è¡ŒåŠ é€Ÿï¼‰
        
        Args:
            data_samples: æ•°æ®æ ·æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆNone=ä½¿ç”¨è®­ç»ƒé…ç½®çš„batch_sizeï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            List[Dict]: æ‰¹é‡é¢„æµ‹ç»“æœ
        """
        if not data_samples:
            self.logger.warning("âš ï¸ ç©ºæ•°æ®æ ·æœ¬åˆ—è¡¨")
            return []
        
        # ä½¿ç”¨è®­ç»ƒé…ç½®çš„batch_sizeä½œä¸ºé»˜è®¤å€¼
        batch_size = batch_size or self.config["train"]["batch_size"]
        all_results = []
        text_col = self.config["data"]["text_col"]
        
        # è¿›åº¦æ¡
        iterator = range(0, len(data_samples), batch_size)
        if show_progress:
            iterator = tqdm(iterator, desc=f"æ‰¹é‡é¢„æµ‹ (batch={batch_size})")
        
        self.model.eval()
        with torch.no_grad():
            for batch_start in iterator:
                batch_end = batch_start + batch_size
                batch_samples = data_samples[batch_start:batch_end]
                
                # æå–æ‰¹é‡æ–‡æœ¬
                batch_texts = []
                valid_indices = []
                for idx, sample in enumerate(batch_samples):
                    if text_col in sample:
                        batch_texts.append(sample[text_col])
                        valid_indices.append(idx)
                    else:
                        self.logger.warning(f"è·³è¿‡æ— æ•ˆæ ·æœ¬ {batch_start+idx}: ç¼ºå°‘{text_col}å­—æ®µ")
                        all_results.append({"error": f"ç¼ºå°‘{text_col}å­—æ®µ", "sample": sample})
                
                if not batch_texts:
                    continue
                
                # æ‰¹é‡Tokenize
                encoded = self.tokenizer(
                    batch_texts,
                    padding='max_length',
                    truncation=True,
                    max_length=self.config["data"]["max_len"],
                    return_tensors='pt'
                )
                
                input_ids = encoded['input_ids'].to(self.device)
                attention_mask = encoded['attention_mask'].to(self.device)
                
                # æ‰¹é‡å‰å‘ä¼ æ’­
                outputs = self.model(input_ids, attention_mask, labels=None)
                
                # æ‰¹é‡åå¤„ç†
                for idx, text_idx in enumerate(valid_indices):
                    sample = batch_samples[text_idx]
                    try:
                        # æå–å•æ¡è¾“å‡º
                        single_output = self._extract_single_output(outputs, idx)
                        result = self._postprocess_result(single_output, sample)
                        all_results.append(result)
                    except Exception as e:
                        error_msg = f"æ ·æœ¬{batch_start+text_idx}å¤„ç†å¤±è´¥: {str(e)}"
                        self.logger.warning(error_msg)
                        all_results.append({"error": error_msg, "sample": sample})
        
        return all_results
    
    def _extract_single_output(self, outputs: Union[Dict[str, torch.Tensor], torch.Tensor], idx: int):
        """ä»æ‰¹é‡è¾“å‡ºä¸­æå–å•æ¡ç»“æœ"""
        if isinstance(outputs, dict):
            return {k: v[idx:idx+1] for k, v in outputs.items()}
        else:
            return outputs[idx:idx+1]
    
    def _postprocess_result(self, outputs: Union[Dict[str, torch.Tensor], torch.Tensor], 
                           data_sample: Dict[str, Any]) -> Dict[str, Any]:
        """åå¤„ç†é¢„æµ‹ç»“æœï¼ˆå…¼å®¹å•/å¤šä»»åŠ¡ï¼‰"""
        if self.task_type == "single_cls":
            return self._process_single_task(outputs, data_sample)
        else:
            return self._process_multi_task(outputs, data_sample)
    
    def _process_single_task(self, outputs: Union[Dict[str, torch.Tensor], torch.Tensor], 
                            data_sample: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•ä»»åŠ¡é¢„æµ‹ç»“æœ"""
        # è·å–logits
        logits = outputs.get("default", outputs) if isinstance(outputs, dict) else outputs
        
        # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
        probs = torch.softmax(logits, dim=-1)
        pred_idx = torch.argmax(probs, dim=-1).cpu().item()
        confidence = probs[0][pred_idx].cpu().item()
        
        # è·å–æ ‡ç­¾æ–‡æœ¬
        label_map = self.label_map["default"]
        pred_label = label_map.get(str(pred_idx), label_map.get(pred_idx, str(pred_idx)))
        
        # æ„å»ºç»“æœ
        result = {
            "text": data_sample.get(self.config["data"]["text_col"], ""),
            "prediction": pred_label,
            "label_id": int(pred_idx),
            "confidence": round(float(confidence), 4),
            "probabilities": {}
        }
        
        # è¾“å‡ºæ‰€æœ‰ç±»åˆ«æ¦‚ç‡ï¼ˆæœ€å¤šæ˜¾ç¤º10ä¸ªï¼‰
        for idx, prob in enumerate(probs[0].cpu().numpy()):
            label = label_map.get(str(idx), label_map.get(idx, str(idx)))
            result["probabilities"][label] = round(float(prob), 4)
            if idx >= 9:
                break
        
        # æ·»åŠ åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if "raw_data" in data_sample:
            result["raw_data"] = data_sample["raw_data"]
        
        return result
    
    def _process_multi_task(self, outputs: Dict[str, torch.Tensor], 
                           data_sample: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å¤šä»»åŠ¡é¢„æµ‹ç»“æœ"""
        if not isinstance(outputs, dict):
            raise ValueError("å¤šä»»åŠ¡æ¨¡å‹å¿…é¡»è¿”å›å­—å…¸æ ¼å¼è¾“å‡º")
        
        result = {
            "text": data_sample.get(self.config["data"]["text_col"], ""),
            "tasks": {}
        }
        
        for task_name, logits in outputs.items():
            if task_name not in self.label_map:
                continue
            
            # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
            probs = torch.softmax(logits, dim=-1)
            pred_idx = torch.argmax(probs, dim=-1).cpu().item()
            confidence = probs[0][pred_idx].cpu().item()
            
            # è·å–ä»»åŠ¡æ ‡ç­¾
            task_label_map = self.label_map[task_name]
            pred_label = task_label_map.get(str(pred_idx), task_label_map.get(pred_idx, str(pred_idx)))
            
            # ä»»åŠ¡ç»“æœ
            result["tasks"][task_name] = {
                "prediction": pred_label,
                "label_id": int(pred_idx),
                "confidence": round(float(confidence), 4),
                "probabilities": {}
            }
            
            # ä»»åŠ¡æ¦‚ç‡
            for idx, prob in enumerate(probs[0].cpu().numpy()):
                label = task_label_map.get(str(idx), task_label_map.get(idx, str(idx)))
                result["tasks"][task_name]["probabilities"][label] = round(float(prob), 4)
                if idx >= 9:
                    break
        
        return result
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "exp_id": self.exp_id,
            "task_type": self.task_type,
            "model_arch": self.config["model"]["arch"],
            "max_length": self.config["data"]["max_len"],
            "label_mapping": self.label_map,
            "device": str(self.device),
            "lora_enabled": hasattr(self.model.bert, "peft_config")
        }
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        self.logger.info("ğŸ”Œ é¢„æµ‹å™¨èµ„æºå·²é‡Šæ”¾")


# # ä¾¿æ·å‡½æ•°ï¼šå¿«é€Ÿåˆ›å»ºé¢„æµ‹å™¨
# def create_predictor(config_path: str, device: Union[str, torch.device, None] = None) -> TextAuditPredictor:
#     """
#     ä»é…ç½®æ–‡ä»¶åˆ›å»ºé¢„æµ‹å™¨
    
#     Args:
#         config_path: é…ç½®æ–‡ä»¶è·¯å¾„
#         device: è®¡ç®—è®¾å¤‡
        
#     Returns:
#         TextAuditPredictorå®ä¾‹
#     """
#     with open(config_path, "r", encoding="utf-8") as f:
#         config = yaml.safe_load(f)
