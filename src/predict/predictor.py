# src/predict/predictor.py
import os
import torch

from typing import Dict, List, Any, Union, Optional
from tqdm import tqdm
from src.utils.logger import get_logger
from src.model.model_factory import TaskTextClassifier, load_tokenizer
from peft import PeftModel

# ä¸è®­ç»ƒä»£ç å¯¹é½çš„é»˜è®¤å‚æ•°ï¼ˆç»Ÿä¸€ç®¡ç†ï¼‰
DEFAULT_CONFIG = {
    "data": {
        "max_len": 256,
        "text_col": "text"
    },
    "model": {
        "arch": "bert-base-chinese",
        "fp16": False
    },
    "train": {
        "batch_size": 20
    },
    "task_type": "single_cls"
}


class TextAuditPredictor:
    """
    æ–‡æœ¬å®¡è®¡é¢„æµ‹å™¨ - å…¼å®¹å•/å¤šä»»åŠ¡ï¼Œä¸è®­ç»ƒä»£ç æ·±åº¦å¯¹é½
    """
    
    def __init__(self, config: Dict[str, Any], device: Union[str, torch.device, None] = None):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸ï¼ˆä¸è®­ç»ƒé…ç½®ä¸€è‡´ï¼‰
            device: é¢„æµ‹è®¾å¤‡ (None=è‡ªåŠ¨é€‰æ‹©, "cuda:0"/"cpu"=æ‰‹åŠ¨æŒ‡å®š)
        """
        # åˆå¹¶é…ç½®ä¸é»˜è®¤å€¼ï¼ˆå…œåº•ï¼‰
        self.config = self._merge_config(config)
        self.exp_id = self.config['exp_id']
        
        # çµæ´»çš„è®¾å¤‡é…ç½®
        self.device = self._setup_device(device)
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = get_logger(f"predict_{self.exp_id}")
        
        # å®éªŒäº§ç‰©ç›®å½•
        self.exp_dir = os.path.join("./outputs", self.exp_id)
        if not os.path.exists(self.exp_dir):
            raise FileNotFoundError(f"å®éªŒç›®å½•ä¸å­˜åœ¨: {self.exp_dir}")
        
        # åŠ è½½å®éªŒäº§ç‰©å’Œæ¨¡å‹
        self._load_experiment_artifacts()
        self._initialize_model()
        
        self.logger.info(f"âœ… é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ | è®¾å¤‡: {self.device} | ä»»åŠ¡ç±»å‹: {self.task_type}")
    
    def _merge_config(self, user_config: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’åˆå¹¶ç”¨æˆ·é…ç½®ä¸é»˜è®¤é…ç½®"""
        def recursive_merge(default: Dict, user: Dict) -> Dict:
            merged = default.copy()
            for k, v in user.items():
                if isinstance(v, dict) and k in merged and isinstance(merged[k], dict):
                    merged[k] = recursive_merge(merged[k], v)
                else:
                    merged[k] = v
            return merged
        
        return recursive_merge(DEFAULT_CONFIG, user_config)
    
    def _setup_device(self, device: Union[str, torch.device, None]) -> torch.device:
        """çµæ´»è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device is None:
            return torch.device("cuda" if torch.cuda.is_available() else "cpu")
        elif isinstance(device, str):
            return torch.device(device)
        elif isinstance(device, torch.device):
            return device
        else:
            raise ValueError(f"æ— æ•ˆçš„è®¾å¤‡ç±»å‹: {type(device)}")
    
    def _validate_config(self):
        """éªŒè¯å…³é”®é…ç½®é¡¹"""
        required_keys = [
            ("exp_id", str),
            ("model", dict),
            ("data", dict)
        ]
        
        for key, expected_type in required_keys:
            if key not in self.config or not isinstance(self.config[key], expected_type):
                raise ValueError(f"é…ç½®ç¼ºå¤±æˆ–ç±»å‹é”™è¯¯: {key} (æœŸæœ›{expected_type.__name__})")
    
    def _load_experiment_artifacts(self):
        """åŠ è½½å®éªŒäº§ç‰©ï¼ˆå…¼å®¹è®­ç»ƒä»£ç çš„ä¿å­˜æ ¼å¼ï¼‰"""
        try:
            # éªŒè¯é…ç½®
            self._validate_config()
            
            # 1. ç¡®å®šä»»åŠ¡ç±»å‹
            self.task_type = self.config.get("task_type", "single_cls")
            
            # 2. å¤„ç†æ ‡ç­¾æ˜ å°„ï¼ˆå¯¹é½è®­ç»ƒä»£ç çš„ä»»åŠ¡åï¼‰
            self.label_map = self.config["data"]["label_mapping"]
            if self.task_type == "single_cls":
                # å•ä»»åŠ¡ä¸‹ï¼šdefaultå¯¹åº”ç¬¬ä¸€ä¸ªæ ‡ç­¾æ˜ å°„
                first_task_key = next(iter(self.label_map.keys()))
                self.label_map = {"default": self.label_map[first_task_key]}
            
            # 3. åŠ è½½tokenizer
            self.tokenizer = self._load_tokenizer()
            
            self.logger.info(f"ğŸ“ å®éªŒäº§ç‰©åŠ è½½æˆåŠŸ | ä»»åŠ¡ç±»å‹: {self.task_type}")
            
        except Exception as e:
            self.logger.error(f"âŒ å®éªŒäº§ç‰©åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_tokenizer(self):
        """åŠ è½½Tokenizerï¼ˆä¼˜å…ˆåŠ è½½è®­ç»ƒå¥½çš„ï¼Œå›é€€åˆ°é…ç½®ï¼‰"""
        tokenizer_path = os.path.join(self.exp_dir, "tokenizer")
        if os.path.exists(tokenizer_path):
            from transformers import AutoTokenizer
            return AutoTokenizer.from_pretrained(tokenizer_path)
        else:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„Tokenizerï¼Œä»é…ç½®åŠ è½½")
            return load_tokenizer(self.config)

    def _initialize_model(self):
        """
        åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¸¥æ ¼åŒ¹é…è®­ç»ƒä»£ç ç»“æ„ï¼‰
        æ–°å¢ï¼šæ¨¡å‹åˆå§‹åŒ–åæ ¡éªŒLoRAæƒé‡å®Œæ•´æ€§
        """
        try:
            # 1. åˆ›å»ºæ¨¡å‹ç»“æ„
            self.model = TaskTextClassifier(self.config, self.tokenizer)
            self.model.to(self.device)

            # 2. åŠ è½½æ¨¡å‹æƒé‡ï¼ˆé¡ºåºï¼šå®Œæ•´æ¨¡å‹ -> LoRA -> åˆ†ç±»å¤´ï¼‰
            self._load_model_weights()

            # 3. è¯„ä¼°/é¢„æµ‹æ¨¡å¼
            self.model.eval()
            self.logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ | è®¾å¤‡: {self.device} | ä»»åŠ¡ç±»å‹: {self.task_type}")

            # ========== æ–°å¢ï¼šæ¨¡å‹åˆå§‹åŒ–åLoRAæƒé‡å…¨å±€æ ¡éªŒ ==========
            self.logger.info("ğŸ” å¼€å§‹å…¨å±€LoRAæƒé‡æ ¡éªŒ...")
            try:
                from peft.utils import get_peft_model_state_dict
                # å°è¯•æå–LoRAæƒé‡
                lora_state_dict = get_peft_model_state_dict(self.model.bert)
                self.logger.info(f"ğŸ“Š LoRAæƒé‡é”®æ€»æ•°: {len(lora_state_dict.keys())}")

                # æ‰“å°å‰5ä¸ªLoRAæƒé‡é”®ï¼ˆç›´è§‚éªŒè¯ï¼‰
                if lora_state_dict:
                    top5_keys = list(lora_state_dict.keys())[:5]
                    for idx, k in enumerate(top5_keys):
                        self.logger.info(f"   ç¬¬{idx + 1}ä¸ªLoRAæƒé‡é”®: {k}")
                else:
                    self.logger.error("âŒ æœªæå–åˆ°ä»»ä½•LoRAæƒé‡é”®ï¼")

            except ImportError:
                self.logger.error("âŒ ç¼ºå°‘peftåº“ï¼Œæ— æ³•æå–LoRAæƒé‡ï¼")
            except AttributeError as e:
                self.logger.error(f"âŒ æ¨¡å‹æ— LoRAç›¸å…³å±æ€§: {e}")
            except Exception as e:
                self.logger.error(f"âŒ æå–LoRAæƒé‡å¤±è´¥: {str(e)}")
            # ======================================================

        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise


    def _load_model_weights(self, torch=None):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ï¼ˆPEFT æ ‡å‡† LoRA æ¨¡å¼ï¼‰
        é¡ºåºï¼š
            1ï¸âƒ£ LoRA é€‚é…å™¨ (adapter_model.safetensors/bin)
            2ï¸âƒ£ åˆ†ç±»å¤´ (classifiers.pt)

        å…³é”®ä¿®æ­£ï¼š
        1. å…¼å®¹åŠ è½½ .bin å’Œ .safetensors æ ¼å¼ã€‚
        2. ç§»é™¤ model.pt åŠ è½½ã€‚
        3. å…³é”®ï¼šè‹¥ self.model.bert å·²æ˜¯ PeftModel (é€šè¿‡ _inject_lora)ï¼Œåˆ™ä½¿ç”¨ load_adapter åŠ è½½æƒé‡ã€‚
        """

        # 1. ç§»é™¤ model.pt åŠ è½½ï¼š
        self.logger.info("â„¹ï¸ LoRAæ¨¡å¼ï¼Œè·³è¿‡å®Œæ•´æ¨¡å‹ model.pt çš„åŠ è½½ã€‚")

        # 2. LoRA é€‚é…å™¨åŠ è½½
        lora_path = self.exp_dir

        # æ ¸å¿ƒä¿®æ­£ï¼šæ£€æŸ¥ adapter_model.safetensors å’Œ adapter_model.bin
        adapter_bin_path = os.path.join(lora_path, "adapter_model.bin")
        adapter_safetensors_path = os.path.join(lora_path, "adapter_model.safetensors")

        # æ£€æŸ¥é€‚é…å™¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(adapter_safetensors_path) or os.path.exists(adapter_bin_path):
            try:
                from peft import PeftModel

                # ã€å…³é”®é€»è¾‘ä¿®æ­£ã€‘
                # å¦‚æœ TaskTextClassifier å·²ç»åœ¨ __init__ ä¸­æ³¨å…¥äº† LoRA ç»“æ„ (PeftModel)ï¼Œ
                # æˆ‘ä»¬åªéœ€è¦åŠ è½½æƒé‡åˆ°è¿™ä¸ªå·²å­˜åœ¨çš„ç»“æ„ä¸­ï¼Œé¿å…åŒé‡åŒ…è£…ã€‚
                if not isinstance(self.model.bert, PeftModel):
                    self.logger.error("âŒ æ¨¡å‹åŠ è½½é”™è¯¯ï¼šself.model.bert ä¸æ˜¯ PeftModel å®ä¾‹ã€‚")
                    self.logger.error("è¯·æ£€æŸ¥ Predictor/Evaluator åˆå§‹åŒ–é€»è¾‘ï¼Œç¡®ä¿ LoRA ç»“æ„å·²é€šè¿‡ _inject_lora æ³¨å…¥ã€‚")
                    # å¦‚æœä¸æ˜¯ PeftModelï¼Œåˆ™å°è¯•ä½¿ç”¨ from_pretrained è¿›è¡ŒåŒ…è£…ï¼ˆè¿™æ˜¯ä¸ºäº†å…¼å®¹æ—§é€»è¾‘ï¼Œä½†ä¼šäº§ç”Ÿè­¦å‘Šï¼‰
                    # ç¡®ä¿ dtype åŒ¹é…
                    dtype_config = self.config["train"].get("mixed_precision")
                    dtype = torch.float16 if dtype_config in ["fp16"] else \
                        torch.bfloat16 if dtype_config in ["bf16"] else torch.float32

                    self.logger.warning(
                        "âš ï¸ æ¨¡å‹æœªé¢„å…ˆåŒ…è£…ã€‚å°è¯•ä½¿ç”¨ PeftModel.from_pretrained è¿›è¡ŒåŠ è½½å’ŒåŒ…è£…ï¼Œå¯èƒ½äº§ç”Ÿ 'Found missing adapter keys' è­¦å‘Šã€‚")
                    self.model.bert = PeftModel.from_pretrained(
                        self.model.bert,
                        lora_path,
                        torch_dtype=dtype
                    )
                else:
                    # æ¨èçš„åŠ è½½æ–¹å¼ï¼šä½¿ç”¨ load_adapter åŠ è½½æƒé‡åˆ°å·²å­˜åœ¨çš„ PeftModel æ§½ä½
                    self.logger.info("âœ… æ£€æµ‹åˆ° PeftModel å®ä¾‹ï¼Œä½¿ç”¨ load_adapter åŠ è½½æƒé‡ã€‚")
                    self.model.bert.load_adapter(lora_path, adapter_name="default")

                # ========== LoRAæƒé‡ç²¾ç»†åŒ–æ ¡éªŒ ==========
                self.logger.info("ğŸ” å¼€å§‹LoRAæƒé‡æœ‰æ•ˆæ€§æ ¡éªŒ...")

                lora_A_count = 0
                lora_B_count = 0
                invalid_lora_A = 0
                untrained_lora_B = 0

                # éå†æ‰€æœ‰å‚æ•°è¿›è¡Œæ ¡éªŒ
                for name, param in self.model.bert.named_parameters():
                    if "lora_A" in name:
                        lora_A_count += 1
                        # æ£€æŸ¥ lora_A æ˜¯å¦æ¥è¿‘é›¶ï¼ˆåŠ è½½å¤±è´¥çš„è¿¹è±¡ï¼‰
                        if param.mean().abs().cpu().item() < 1e-8:
                            invalid_lora_A += 1
                            self.logger.warning(
                                f"âš ï¸ LoRAæƒé‡ {name} å‡å€¼æ¥è¿‘é›¶ ({param.mean().abs().cpu().item():.8f})ï¼Œå¯èƒ½åŠ è½½å¤±è´¥ã€‚")

                    elif "lora_B" in name:
                        lora_B_count += 1
                        # æ£€æŸ¥ lora_B æ˜¯å¦æ¥è¿‘é›¶ï¼ˆè®­ç»ƒä¸å……åˆ†çš„è­¦ç¤ºï¼‰
                        if param.mean().abs().cpu().item() < 1e-6:
                            untrained_lora_B += 1

                # æ€»ç»“ä¸åˆ¤æ–­
                if lora_A_count == 0:
                    self.logger.error("âŒ LoRAé€‚é…å™¨åŠ è½½å¤±è´¥ï¼šæœªæ£€æµ‹åˆ°ä»»ä½• lora_A æƒé‡ï¼")
                elif invalid_lora_A > 0:
                    self.logger.error(f"âŒ LoRAé€‚é…å™¨åŠ è½½å¤±è´¥ï¼š{invalid_lora_A} ä¸ª lora_A æƒé‡å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥è®­ç»ƒä¿å­˜é€»è¾‘ï¼")
                else:
                    self.logger.info("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸï¼ˆæ‰€æœ‰ lora_A æƒé‡æœ‰æ•ˆï¼‰")
                    if untrained_lora_B == lora_B_count and lora_B_count > 0:
                        self.logger.warning(
                            f"âš ï¸ LoRA-B ({lora_B_count}ä¸ª) æƒé‡å‡å€¼ä»æ¥è¿‘ 0ï¼Œæ¨¡å‹å¯èƒ½æ¬ æ‹Ÿåˆæˆ–è®­ç»ƒæ­¥æ•°è¿‡å°‘ã€‚")

                # ==============================================

            except ImportError:
                self.logger.error("âŒ LoRAåŠ è½½å¤±è´¥ï¼šç¼ºå°‘peftåº“ï¼Œè¯·æ‰§è¡Œ pip install peft")
            except Exception as e:
                self.logger.error(f"âš ï¸ LoRAé€‚é…å™¨åŠ è½½å¤±è´¥: {str(e)}")
        else:
            self.logger.warning(f"âš ï¸ æ ‡å‡†PEFTé€‚é…å™¨æ–‡ä»¶ (adapter_model.bin æˆ– .safetensors) ä¸å­˜åœ¨äº {lora_path}")

        # 3. åˆ†ç±»å¤´åŠ è½½ (é€»è¾‘ä¸å˜)
        classifier_path = os.path.join(self.exp_dir, "classifiers.pt")
        if os.path.exists(classifier_path):
            try:
                # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿ torch åœ¨æ­¤ä½œç”¨åŸŸå¯ç”¨
                import torch

                clf_state = torch.load(classifier_path, map_location=self.device)
                self.model.classifiers.load_state_dict(clf_state)
                self.logger.info("âœ… åˆ†ç±»å¤´æƒé‡åŠ è½½æˆåŠŸ")
            except Exception as e:
                self.logger.error(f"âš ï¸ åˆ†ç±»å¤´åŠ è½½å¤±è´¥: {e}")
        else:
            self.logger.warning(f"âš ï¸ åˆ†ç±»å¤´æƒé‡æ–‡ä»¶ {classifier_path} ä¸å­˜åœ¨ã€‚")
        # ç¡®ä¿æ¨¡å‹åœ¨è®¾å¤‡ä¸Š
        self.model.to(self.device)
        self.model.eval()


    # def _load_model_weights(self):
    #     """
    #     åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ï¼ˆè®­ç»ƒä¿å­˜é¡ºåºå…¼å®¹æ€§ä¼˜åŒ–ï¼‰
    #     é¡ºåºï¼š
    #         1ï¸âƒ£ å®Œæ•´æ¨¡å‹ model.ptï¼ˆè¦†ç›– base+classifierï¼‰
    #         2ï¸âƒ£ LoRA é€‚é…å™¨ lora_adapterï¼ˆè¦†ç›– Bert å±‚ï¼‰
    #         3ï¸âƒ£ åˆ†ç±»å¤´ classifiers.ptï¼ˆè¦†ç›– classifierï¼‰
    #     æ–°å¢ï¼šLoRAåŠ è½½åé€æƒé‡æ ¡éªŒï¼ˆæ˜¯å¦å­˜åœ¨/æ˜¯å¦éšæœºï¼‰
    #     """
    #
    #     # å®Œæ•´æ¨¡å‹
    #     model_path = os.path.join(self.exp_dir, "model.pt")
    #     if os.path.exists(model_path):
    #         try:
    #             state_dict = torch.load(model_path, map_location=self.device)
    #             self.model.load_state_dict(state_dict)
    #             self.logger.info("âœ… å®Œæ•´æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
    #         except Exception as e:
    #             self.logger.warning(f"âš ï¸ åŠ è½½å®Œæ•´æ¨¡å‹å¤±è´¥: {e}")
    #
    #     # LoRA é€‚é…å™¨
    #     lora_path = os.path.join(self.exp_dir, "lora_adapter")
    #     if os.path.exists(lora_path):
    #         try:
    #             from peft import PeftModel  # ç¡®ä¿å¯¼å…¥PeftModel
    #             dtype = torch.float16 if self.config["model"].get("fp16", False) else torch.float32
    #             self.model.bert = PeftModel.from_pretrained(
    #                 self.model.bert,
    #                 lora_path,
    #                 torch_dtype=dtype
    #             )
    #
    #             # ========== æ–°å¢ï¼šLoRAæƒé‡ç²¾ç»†åŒ–æ ¡éªŒ ==========
    #             self.logger.info("ğŸ” å¼€å§‹LoRAæƒé‡æœ‰æ•ˆæ€§æ ¡éªŒ...")
    #             has_lora_weights = False
    #             random_lora_count = 0
    #             total_lora_count = 0
    #             invalid_lora_keys = []
    #
    #             # éå†æ‰€æœ‰å‚æ•°ï¼Œæ£€æŸ¥LoRAæ ¸å¿ƒæƒé‡
    #             for name, param in self.model.bert.named_parameters():
    #                 if "lora_A" in name or "lora_B" in name:
    #                     total_lora_count += 1
    #                     has_lora_weights = True
    #                     # è®¡ç®—æƒé‡å‡å€¼ï¼ˆåˆ¤æ–­æ˜¯å¦ä¸ºéšæœºåˆå§‹åŒ–ï¼‰
    #                     param_mean = param.mean().abs().cpu().item()
    #                     if param_mean < 1e-6:
    #                         random_lora_count += 1
    #                         invalid_lora_keys.append(name)
    #                         self.logger.warning(f"âš ï¸ LoRAæƒé‡ {name} ä¸ºéšæœºå€¼ï¼ˆå‡å€¼ï¼š{param_mean:.8f}ï¼‰")
    #                     else:
    #                         self.logger.info(f"âœ… LoRAæƒé‡ {name} æœ‰æ•ˆï¼ˆå‡å€¼ï¼š{param_mean:.4f}ï¼‰")
    #
    #             # è¾“å‡ºæ ¡éªŒæ€»ç»“
    #             self.logger.info(f"\nğŸ“Š LoRAæƒé‡æ ¡éªŒæ€»ç»“:")
    #             self.logger.info(f"   æ£€æµ‹åˆ°LoRAæƒé‡æ€»æ•°: {total_lora_count}")
    #             self.logger.info(f"   éšæœºåˆå§‹åŒ–æƒé‡æ•°: {random_lora_count}")
    #             self.logger.info(f"   æœ‰æ•ˆæƒé‡æ•°: {total_lora_count - random_lora_count}")
    #
    #             # ä¿®æ­£æ—¥å¿—æç¤ºï¼šåŸºäºå®é™…æ ¡éªŒç»“æœåˆ¤æ–­æ˜¯å¦çœŸçš„åŠ è½½æˆåŠŸ
    #             if total_lora_count == 0:
    #                 self.logger.error("âŒ LoRAé€‚é…å™¨åŠ è½½å¤±è´¥ï¼šæœªæ£€æµ‹åˆ°ä»»ä½•lora_A/lora_Bæƒé‡ï¼")
    #             elif random_lora_count == total_lora_count:
    #                 self.logger.error("âŒ LoRAé€‚é…å™¨åŠ è½½å¤±è´¥ï¼šæ‰€æœ‰æƒé‡å‡ä¸ºéšæœºåˆå§‹åŒ–ï¼")
    #             elif random_lora_count > 0:
    #                 self.logger.warning(
    #                     f"âš ï¸ LoRAé€‚é…å™¨éƒ¨åˆ†å¤±æ•ˆï¼š{random_lora_count}ä¸ªæƒé‡ä¸ºéšæœºå€¼ï¼ˆ{invalid_lora_keys[:3]}...ï¼‰")
    #             else:
    #                 self.logger.info("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸï¼ˆæ‰€æœ‰æƒé‡æœ‰æ•ˆï¼‰")
    #             # ==============================================
    #
    #         except ImportError:
    #             self.logger.error("âŒ LoRAåŠ è½½å¤±è´¥ï¼šç¼ºå°‘peftåº“ï¼Œè¯·æ‰§è¡Œ pip install peft")
    #         except Exception as e:
    #             self.logger.error(f"âš ï¸ LoRAåŠ è½½å¤±è´¥: {str(e)}")
    #     else:
    #         self.logger.warning(f"âš ï¸ LoRAé€‚é…å™¨ç›®å½•ä¸å­˜åœ¨: {lora_path}")
    #
    #     # åˆ†ç±»å¤´
    #     classifier_path = os.path.join(self.exp_dir, "classifiers.pt")
    #     if os.path.exists(classifier_path):
    #         try:
    #             clf_state = torch.load(classifier_path, map_location=self.device)
    #             self.model.classifiers.load_state_dict(clf_state)
    #             self.logger.info("âœ… åˆ†ç±»å¤´æƒé‡åŠ è½½æˆåŠŸ")
    #         except Exception as e:
    #             self.logger.error(f"âš ï¸ åˆ†ç±»å¤´åŠ è½½å¤±è´¥: {e}")
    #
    #     # ç¡®ä¿æ¨¡å‹åœ¨è®¾å¤‡ä¸Š
    #     self.model.to(self.device)
    #     self.model.eval()


    def predict(self, data_sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        é¢„æµ‹å•æ¡æ•°æ®
        
        Args:
            data_sample: åŒ…å«textå­—æ®µçš„å­—å…¸
            
        Returns:
            Dict: é¢„æµ‹ç»“æœï¼ˆå«æ ‡ç­¾ã€ç½®ä¿¡åº¦ç­‰ï¼‰
        """
        # è¾“å…¥éªŒè¯
        text_col = self.config["data"]["text_col"]
        if text_col not in data_sample:
            raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {text_col}")
        
        text = data_sample[text_col]
        
        # Tokenize
        encoded = self.tokenizer.encode_plus(
            text,
            padding='max_length',
            truncation=True,
            max_length=self.config["data"]["max_len"],
            return_tensors='pt',
            return_attention_mask=True
        )
        
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = self.model(input_ids, attention_mask, labels=None)

        # åå¤„ç†ç»“æœ
        return self._postprocess_result(outputs, data_sample)
    
    def predict_batch(self, data_samples: List[Dict[str, Any]],
                     batch_size: int = None, show_progress: bool = True) -> List[Dict[str, Any]]:
        """
        é«˜æ•ˆæ‰¹é‡é¢„æµ‹ï¼ˆGPUå¹¶è¡ŒåŠ é€Ÿï¼‰

        Args:
            data_samples: æ•°æ®æ ·æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆNone=ä½¿ç”¨è®­ç»ƒé…ç½®çš„batch_sizeï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡

        Returns:
            List[Dict]: æ‰¹é‡é¢„æµ‹ç»“æœ
        """
        if not data_samples:
            self.logger.warning("âš ï¸ ç©ºæ•°æ®æ ·æœ¬åˆ—è¡¨")
            return []

        # ä½¿ç”¨è®­ç»ƒé…ç½®çš„batch_sizeä½œä¸ºé»˜è®¤å€¼
        batch_size = batch_size or self.config["train"]["batch_size"]
        all_results = []
        text_col = self.config["data"]["text_col"]

        # è¿›åº¦æ¡
        iterator = range(0, len(data_samples), batch_size)
        if show_progress:
            iterator = tqdm(iterator, desc=f"æ‰¹é‡é¢„æµ‹ (batch={batch_size})")

        self.model.eval()
        with torch.no_grad():
            for batch_start in iterator:
                batch_end = batch_start + batch_size
                batch_samples = data_samples[batch_start:batch_end]

                # æå–æ‰¹é‡æ–‡æœ¬
                batch_texts = []
                valid_indices = []
                for idx, sample in enumerate(batch_samples):
                    if text_col in sample:
                        batch_texts.append(sample[text_col])
                        valid_indices.append(idx)
                    else:
                        self.logger.warning(f"è·³è¿‡æ— æ•ˆæ ·æœ¬ {batch_start+idx}: ç¼ºå°‘{text_col}å­—æ®µ")
                        all_results.append({"error": f"ç¼ºå°‘{text_col}å­—æ®µ", "sample": sample})

                if not batch_texts:
                    continue

                # æ‰¹é‡Tokenize
                encoded = self.tokenizer(
                    batch_texts,
                    padding='max_length',
                    truncation=True,
                    max_length=self.config["data"]["max_len"],
                    return_tensors='pt'
                )

                input_ids = encoded['input_ids'].to(self.device)
                attention_mask = encoded['attention_mask'].to(self.device)

                # æ‰¹é‡å‰å‘ä¼ æ’­
                outputs = self.model(input_ids, attention_mask, labels=None)

                # æ‰¹é‡åå¤„ç†
                for idx, text_idx in enumerate(valid_indices):
                    sample = batch_samples[text_idx]
                    try:
                        # æå–å•æ¡è¾“å‡º
                        single_output = self._extract_single_output(outputs, idx)
                        result = self._postprocess_result(single_output, sample)
                        all_results.append(result)
                    except Exception as e:
                        error_msg = f"æ ·æœ¬{batch_start+text_idx}å¤„ç†å¤±è´¥: {str(e)}"
                        self.logger.warning(error_msg)
                        all_results.append({"error": error_msg, "sample": sample})

        return all_results

    def _extract_single_output(self, outputs: Union[Dict[str, torch.Tensor], torch.Tensor], idx: int):
        """ä»æ‰¹é‡è¾“å‡ºä¸­æå–å•æ¡ç»“æœ"""
        if isinstance(outputs, dict):
            return {k: v[idx:idx+1] for k, v in outputs.items()}
        else:
            return outputs[idx:idx+1]

    def _postprocess_result(self, outputs: Union[Dict[str, torch.Tensor], torch.Tensor], 
                           data_sample: Dict[str, Any]) -> Dict[str, Any]:
        """åå¤„ç†é¢„æµ‹ç»“æœï¼ˆå…¼å®¹å•/å¤šä»»åŠ¡ï¼‰"""
        if self.task_type == "single_cls":
            return self._process_single_task(outputs, data_sample)
        else:
            return self._process_multi_task(outputs, data_sample)
    
    def _process_single_task(self, outputs: Union[Dict[str, torch.Tensor], torch.Tensor], 
                            data_sample: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å•ä»»åŠ¡é¢„æµ‹ç»“æœ"""
        # è·å–logits
        logits = outputs.get("default", outputs) if isinstance(outputs, dict) else outputs
        
        # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
        probs = torch.softmax(logits, dim=-1)
        pred_idx = torch.argmax(probs, dim=-1).cpu().item()
        confidence = probs[0][pred_idx].cpu().item()
        
        # è·å–æ ‡ç­¾æ–‡æœ¬
        label_map = self.label_map["default"]
        pred_label = label_map.get(str(pred_idx), label_map.get(pred_idx, str(pred_idx)))
        
        # æ„å»ºç»“æœ
        result = {
            "text": data_sample.get(self.config["data"]["text_col"], ""),
            "prediction": pred_label,
            "label_id": int(pred_idx),
            "confidence": round(float(confidence), 4),
            "probabilities": {}
        }
        
        # è¾“å‡ºæ‰€æœ‰ç±»åˆ«æ¦‚ç‡ï¼ˆæœ€å¤šæ˜¾ç¤º10ä¸ªï¼‰
        for idx, prob in enumerate(probs[0].cpu().numpy()):
            label = label_map.get(str(idx), label_map.get(idx, str(idx)))
            result["probabilities"][label] = round(float(prob), 4)
            if idx >= 9:
                break
        
        # æ·»åŠ åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if "raw_data" in data_sample:
            result["raw_data"] = data_sample["raw_data"]
        
        return result
    
    def _process_multi_task(self, outputs: Dict[str, torch.Tensor], 
                           data_sample: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å¤šä»»åŠ¡é¢„æµ‹ç»“æœ"""
        if not isinstance(outputs, dict):
            raise ValueError("å¤šä»»åŠ¡æ¨¡å‹å¿…é¡»è¿”å›å­—å…¸æ ¼å¼è¾“å‡º")
        
        result = {
            "text": data_sample.get(self.config["data"]["text_col"], ""),
            "tasks": {}
        }
        
        for task_name, logits in outputs.items():
            if task_name not in self.label_map:
                continue
            
            # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
            probs = torch.softmax(logits, dim=-1)
            pred_idx = torch.argmax(probs, dim=-1).cpu().item()
            confidence = probs[0][pred_idx].cpu().item()
            
            # è·å–ä»»åŠ¡æ ‡ç­¾
            task_label_map = self.label_map[task_name]
            pred_label = task_label_map.get(str(pred_idx), task_label_map.get(pred_idx, str(pred_idx)))
            
            # ä»»åŠ¡ç»“æœ
            result["tasks"][task_name] = {
                "prediction": pred_label,
                "label_id": int(pred_idx),
                "confidence": round(float(confidence), 4),
                "probabilities": {}
            }
            
            # ä»»åŠ¡æ¦‚ç‡
            for idx, prob in enumerate(probs[0].cpu().numpy()):
                label = task_label_map.get(str(idx), task_label_map.get(idx, str(idx)))
                result["tasks"][task_name]["probabilities"][label] = round(float(prob), 4)
                if idx >= 9:
                    break
        
        return result
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "exp_id": self.exp_id,
            "task_type": self.task_type,
            "model_arch": self.config["model"]["arch"],
            "max_length": self.config["data"]["max_len"],
            "label_mapping": self.label_map,
            "device": str(self.device),
            "lora_enabled": hasattr(self.model.bert, "peft_config")
        }
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        self.logger.info("ğŸ”Œ é¢„æµ‹å™¨èµ„æºå·²é‡Šæ”¾")
