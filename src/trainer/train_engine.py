import os
import torch
import torch.nn as nn
from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup
from src.utils.logger import get_logger
from src.trainer.metric_manager import MetricManager

class Trainer:
    def __init__(self, config, model, train_loader, dev_loader, tokenizer):
        self.config = config
        self.logger = get_logger(config["exp_id"])
        self.model = model
        self.train_loader = train_loader
        self.dev_loader = dev_loader
        self.tokenizer = tokenizer # éœ€è¦ä¿å­˜ tokenizer
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        
        # è¾“å‡ºè·¯å¾„
        self.output_dir = os.path.join("outputs", config["exp_id"])
        self.lora_save_path = os.path.join(self.output_dir, "lora_adapter")
        self.tokenizer_save_path = os.path.join(self.output_dir, "tokenizer")
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.optimizer = self._build_optimizer()
        self.scheduler = self._build_scheduler()
        self.metric_manager = MetricManager(config)

    def _build_optimizer(self):
        """
        ç§»æ¤æ—§ä»£ç çš„æ ¸å¿ƒä¼˜åŒ–å™¨åˆ†ç»„é€»è¾‘
        """
        # 1. ç­›é€‰å‚æ•°
        lora_params = [p for n, p in self.model.named_parameters() if "lora_" in n and p.requires_grad]
        classifier_params = [p for n, p in self.model.named_parameters() if "classifiers" in n and p.requires_grad]
        bert_params = [p for n, p in self.model.named_parameters() 
                    if "lora_" not in n and "classifiers" not in n and p.requires_grad]

        # 2. æ„å»ºåˆ†ç»„
        optimizer_grouped_parameters = []
        
        # ä»çš„é…ç½®è·¯å¾„è¯»å–å­¦ä¹ ç‡
        lr_config = self.config["train"]["optimizer"]["groups"]
        
        # LoRA ç»„
        if lora_params:
            optimizer_grouped_parameters.append({
                "params": lora_params,
                "lr": float(lr_config["lora"]),
                "weight_decay": 0.01
            })
            
        # Classifier ç»„
        if classifier_params:
            optimizer_grouped_parameters.append({
                "params": classifier_params,
                "lr": float(lr_config["classifier"]),
                "weight_decay": 0.01
            })
            
        # Bert ç»„ (é€šå¸¸ä¸ºç©ºï¼Œå› ä¸ºå†»ç»“äº†)
        if bert_params:
            optimizer_grouped_parameters.append({
                "params": bert_params,
                "lr": float(lr_config["bert"]),
                "weight_decay": 0.001
            })
            
        self.logger.info(f"ä¼˜åŒ–å™¨ç»„: LoRA({len(lora_params)}), Clf({len(classifier_params)}), Bert({len(bert_params)})")
        return torch.optim.AdamW(optimizer_grouped_parameters)

    def _build_scheduler(self):
        num_epochs = self.config["train"]["num_epochs"]
        grad_accum = self.config["train"].get("gradient_accumulation_steps", 1)
        total_steps = len(self.train_loader) * num_epochs // grad_accum
        warmup_ratio = self.config["train"].get("warmup_ratio", 0.1)
        
        return get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=int(total_steps * warmup_ratio),
            num_training_steps=total_steps
        )

    def train(self):
        epochs = self.config["train"]["num_epochs"]
        grad_accum = self.config["train"].get("gradient_accumulation_steps", 1)
        best_score = float('-inf')
        global_step = 0
        
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        
        for epoch in range(1, epochs + 1):
            self.model.train()
            epoch_loss = 0.0
            
            pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}/{epochs}")
            self.optimizer.zero_grad()
            
            for step, batch in enumerate(pbar):
                # ç§»åŠ¨æ•°æ®
                input_ids = batch["input_ids"].to(self.device)
                mask = batch["attention_mask"].to(self.device)
                labels = batch["labels"]
                if isinstance(labels, dict):
                    labels = {k: v.to(self.device) for k, v in labels.items()}
                else:
                    labels = labels.to(self.device)
                
                # Forward - å®‰å…¨å¤„ç†è¿”å›å€¼
                result = self.model(input_ids, mask, labels)
                
                # æ£€æŸ¥è¿”å›å€¼ç±»å‹
                if isinstance(result, tuple) and len(result) == 2:
                    # å¦‚æœè¿”å› (loss, logits_dict)
                    loss, logits_dict = result
                    # è¿™é—®é¢˜å…ˆå¤‡æ³¨ä¸€ä¸‹å­ï¼Œåç»­æœ‰æ—¶é—´äº†å†è¯´  
                    # logits_dictæœªè¢«ä½¿ç”¨ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å–äº†æ¨¡å‹çš„logitsè¾“å‡ºï¼Œä½†æ²¡æœ‰ç”¨äºä»»ä½•å®é™…ç”¨é€”
                    # ç¼ºä¹è®­ç»ƒç›‘æ§ï¼šæ— æ³•å®æ—¶æŸ¥çœ‹æ¨¡å‹é¢„æµ‹æ•ˆæœå’Œç½®ä¿¡åº¦
                    # è°ƒè¯•å›°éš¾ï¼šæ— æ³•åˆ†ææ¨¡å‹åœ¨å­¦ä¹ ä»€ä¹ˆ
                elif isinstance(result, torch.Tensor):
                    # å¦‚æœåªè¿”å› loss å¼ é‡
                    loss = result
                    logits_dict = None
                else:
                    # å¤„ç†æ„å¤–æƒ…å†µ
                    raise ValueError(f"æ¨¡å‹è¿”å›äº†æ„å¤–çš„ç±»å‹: {type(result)}")

                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / grad_accum
                loss.backward()
                
                epoch_loss += loss.item()
                
                if (step + 1) % grad_accum == 0:
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    # ã€å…³é”®éªŒè¯ã€‘æ¯100æ­¥æ‰“å°lora_Bæƒé‡å‡å€¼ï¼ˆç¡®ä¿é0ï¼‰
                    if step % 100 == 0:
                        for name, param in self.model.bert.named_parameters():
                            if "lora_B" in name and param.requires_grad:
                                lora_b_mean = torch.mean(param.data).item()
                                self.logger.info(f"ğŸ“Œ Step {step} | {name} å‡å€¼: {lora_b_mean:.6f}")
                                break  # åªæ‰“å°ä¸€ä¸ªlora_Bå‚æ•°å³å¯

                    global_step += 1

                pbar.set_postfix({"loss": f"{loss.item() * grad_accum:.4f}"})
            
            # è¯„ä¼°
            metrics = self.evaluate()
            log_msg = f"Epoch {epoch} | " + " | ".join([f"{k}: {v:.4f}" for k, v in metrics.items()])
            self.logger.info(log_msg)
            
            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            current_score = metrics.get("main_score", 0)
            if current_score > best_score:
                best_score = current_score
                self.save_model()
                self.logger.info(f">>> æ–°æ¨¡å‹è¯ç”Ÿ! åˆ†æ•°: {best_score:.4f}")

    def evaluate(self):
        self.model.eval()
        all_logits = {}
        all_labels = {}
        
        # å…³é”®ä¿®æ”¹ï¼šä»label_colè·å–ä»»åŠ¡åç§°
        if self.config["task_type"] == "single_cls":
            task_names = ["default"]
            # è·å–å•ä»»åŠ¡çš„æ ‡ç­¾é”®ï¼ˆæ¯”å¦‚'misreport'ï¼‰
            label_col_config = self.config["data"]["label_col"]
            if isinstance(label_col_config, dict):
                single_task_key = next(iter(label_col_config.keys()))  # æ‹¿åˆ°'æ ‡ç­¾'
            else:
                single_task_key = label_col_config
        else:
            label_col_config = self.config["data"]["label_col"]
            if isinstance(label_col_config, dict):
                task_names = list(label_col_config.keys())
            else:
                task_names = ["misreport", "risk"]
        
        for t in task_names:
            all_logits[t] = []
            all_labels[t] = []
            
        with torch.no_grad():
            for batch in self.dev_loader:
                input_ids = batch["input_ids"].to(self.device)
                mask = batch["attention_mask"].to(self.device)
                labels = batch["labels"]
                
                outputs = self.model(input_ids, mask, labels=None)
                
                for t in task_names:
                    if self.config["task_type"] == "single_cls":
                        logit = outputs.get("default", outputs) if isinstance(outputs, dict) else outputs
                        # å¤„ç†å­—å…¸æ ¼å¼çš„labels
                        if isinstance(labels, dict):
                            label = labels[single_task_key]  # æå–å¼ é‡ï¼ˆå¦‚labels['misreport']ï¼‰
                        else:
                            label = labels
                    else:
                        logit = outputs[t]
                        label = labels[t]
                        
                    all_logits[t].append(logit.cpu())
                    all_labels[t].append(label.cpu())  # ç°åœ¨labelæ˜¯å¼ é‡ï¼Œå¯è°ƒç”¨cpu()

        for t in task_names:
            all_logits[t] = torch.cat(all_logits[t], dim=0)
            all_labels[t] = torch.cat(all_labels[t], dim=0)

        return self.metric_manager.compute(all_logits, all_labels)

    
    def save_model(self):
        """
        [PEFTæ ‡å‡†] åªä¿å­˜ LoRA é€‚é…å™¨ (adapter_model.bin) å’Œ åˆ†ç±»å¤´ (classifiers.pt)
        """
        # 1. ä¿å­˜ LoRA é€‚é…å™¨ (å…³é”®ï¼šç›´æ¥ç”¨ output_dir)
        # è¿™ä¸€æ­¥ä¼šç”Ÿæˆ adapter_model.bin å’Œ adapter_config.json
        self.model.bert.save_pretrained(self.output_dir)
        self.logger.info(f"âœ… LoRA é€‚é…å™¨æƒé‡å·²ä¿å­˜è‡³: {self.output_dir}")

        # 2. ä¿å­˜ Tokenizer
        self.tokenizer.save_pretrained(self.tokenizer_save_path)

        # 3. ä¿å­˜ Classifiers (state_dict)
        clf_path = os.path.join(self.output_dir, "classifiers.pt")
        torch.save(self.model.classifiers.state_dict(), clf_path)
        self.logger.info(f"âœ… åˆ†ç±»å¤´æƒé‡å·²ä¿å­˜è‡³: {clf_path}")
