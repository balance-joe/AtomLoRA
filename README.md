# AtomLoRA - é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

**AtomLoRA** æ˜¯ä¸€ä¸ªåŸºäº **LoRA (Low-Rank Adaptation)** æŠ€æœ¯çš„è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œä¸“ä¸ºå¿«é€Ÿè¿­ä»£å’Œé«˜æ•ˆè®­ç»ƒè€Œè®¾è®¡ã€‚è¯¥é¡¹ç›®é€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹å¼ï¼Œèƒ½å¤Ÿåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®ç°å¯¹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚BERTã€ERNIEç­‰ï¼‰çš„å¿«é€Ÿé€‚é…ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **å‚æ•°é«˜æ•ˆå¾®è°ƒ (LoRA)**ï¼šä»…éœ€å¾®è°ƒ0.01%-1%çš„å‚æ•°ï¼Œæ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å’Œæ˜¾å­˜å ç”¨
- **å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶**ï¼šæ”¯æŒå•ä»»åŠ¡å’Œå¤šä»»åŠ¡åˆ†ç±»ï¼Œçµæ´»çš„ä»»åŠ¡é…ç½®ç³»ç»Ÿ
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„ä»£ç ç»“æ„ï¼Œæ˜“äºæ‰©å±•å’Œå®šåˆ¶
- **é…ç½®é©±åŠ¨çš„å¼€å‘**ï¼šåŸºäºYAMLçš„çµæ´»é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒé…ç½®ç»§æ‰¿
- **å®Œæ•´çš„è®­ç»ƒæµç¨‹**ï¼šä»æ•°æ®åŠ è½½ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒåˆ°è¯„ä¼°çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆ
- **ç”Ÿäº§çº§APIæœåŠ¡**ï¼šå†…ç½®FastAPIæœåŠ¡ï¼Œæ”¯æŒæ¨¡å‹çƒ­åŠ è½½å’Œåœ¨çº¿æ¨ç†

---

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
AtomLoRA/
â”œâ”€â”€ api/                          # FastAPI åº”ç”¨å±‚
â”‚   â”œâ”€â”€ app.py                    # REST API ç«¯ç‚¹å®šä¹‰
â”‚   â”œâ”€â”€ model_manager.py          # æ¨¡å‹ç®¡ç†å’Œæ¨ç†æ¥å£
â”‚   â””â”€â”€ util.py                   # å·¥å…·å‡½æ•°ï¼ˆå“åº”æ ¼å¼åŒ–ï¼‰
â”‚
â”œâ”€â”€ configs/                      # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ templates/                # é…ç½®æ¨¡æ¿ï¼ˆåŸºç¡€é…ç½®ï¼‰
â”‚   â”‚   â”œâ”€â”€ bert_lora_template.yaml
â”‚   â”‚   â””â”€â”€ ernie_lora_template.yaml
â”‚   â””â”€â”€ *.yaml                    # å…·ä½“å®éªŒé…ç½®
â”‚
â”œâ”€â”€ data/                         # æ•°æ®é›†å­˜å‚¨
â”‚   â””â”€â”€ raw/                      # åŸå§‹æ•°æ®
â”‚
â”œâ”€â”€ models/                       # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ bert-base-chinese/
â”‚   â””â”€â”€ ernie-3.0-base-zh/
â”‚
â”œâ”€â”€ src/                          # æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ parser.py             # é…ç½®è§£æå™¨
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ model_factory.py      # æ¨¡å‹å·¥å‚ï¼ˆæ„å»ºTaskTextClassifierï¼‰
â”‚   â”‚   â”œâ”€â”€ lora_adapter.py       # LoRAé€‚é…å™¨
â”‚   â”‚   â””â”€â”€ text_dataset.py       # æ•°æ®é›†å’ŒDataLoader
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_processor.py     # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ trainer/
â”‚   â”‚   â”œâ”€â”€ train_engine.py       # è®­ç»ƒå¼•æ“
â”‚   â”‚   â””â”€â”€ metric_manager.py     # è¯„ä¼°æŒ‡æ ‡ç®¡ç†
â”‚   â”œâ”€â”€ predict/
â”‚   â”‚   â””â”€â”€ predictor.py          # æ¨ç†é¢„æµ‹
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py             # æ—¥å¿—ç³»ç»Ÿ
â”‚
â”œâ”€â”€ outputs/                      # æ¨¡å‹è¾“å‡º
â”‚   â””â”€â”€ {exp_id}/
â”‚       â”œâ”€â”€ lora_adapter/         # LoRAæƒé‡
â”‚       â”œâ”€â”€ classifiers.pt        # åˆ†ç±»å¤´æƒé‡
â”‚       â””â”€â”€ tokenizer/            # åˆ†è¯å™¨
â”‚
â”œâ”€â”€ logs/                         # è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ main.py                       # è®­ç»ƒå…¥å£
â”œâ”€â”€ evaluator.py                  # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ predict.py                    # æ¨ç†è„šæœ¬
â””â”€â”€ requirements.txt              # ä¾èµ–åŒ…
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/balance-joe/AtomLoRA.git
cd AtomLoRA

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ä¸»è¦ä¾èµ–ï¼š
# - transformers >= 4.25.0    # HuggingFaceæ¨¡å‹åº“
# - peft >= 0.4.0             # LoRAæ¡†æ¶
# - torch >= 1.12.0           # æ·±åº¦å­¦ä¹ æ¡†æ¶
# - fastapi                   # Web APIæ¡†æ¶
```

### 2. æ•°æ®å‡†å¤‡

å‡†å¤‡JSONLæ ¼å¼çš„æ•°æ®é›†ï¼š

```jsonl
{"content": "æ ·æœ¬æ–‡æœ¬", "label": 0}
{"content": "æ ·æœ¬æ–‡æœ¬", "label": 1}
```

å°†æ•°æ®æ”¾åœ¨ `data/raw/` ç›®å½•ä¸‹ï¼Œå¦‚ `train_default.jsonl`ã€‚

### 3. é…ç½®å®éªŒ

æŸ¥çœ‹ `configs/` ç›®å½•ä¸‹çš„æ¨¡æ¿é…ç½®ï¼Œåˆ›å»ºè‡ªå·±çš„é…ç½®æ–‡ä»¶ï¼š

```yaml
# ç¤ºä¾‹ï¼šconfigs/my_experiment.yaml
exp_id: "my_experiment"
description: "æˆ‘çš„åˆ†ç±»å®éªŒ"
task_type: "single_cls"  # æˆ– "multi_cls"

model:
  arch: "bert-base-chinese"
  path: "./models/bert-base-chinese"
  freeze_bert: False
  lora:
    enabled: True
    rank: 8           # LoRAç§©å¤§å°
    alpha: 16         # ç¼©æ”¾ç³»æ•°
    dropout: 0.05
    target_modules: ["query", "key", "value"]
    bias: "none"

data:
  train_path: "./data/raw/train_default.jsonl"
  dev_path: "./data/raw/dev_default.jsonl"
  max_len: 256
  label_col: "label"

train:
  num_epochs: 10
  batch_size: 32
  gradient_accumulation_steps: 2
  warmup_ratio: 0.1
  
  optimizer:
    type: "AdamW"
    groups:
      bert: 2e-6
      lora: 1.5e-4
      classifier: 5e-4
```

### 4. å¼€å§‹è®­ç»ƒ

```bash
# ä¿®æ”¹ main.py ä¸­çš„é…ç½®è·¯å¾„ï¼Œæˆ–ç›´æ¥è¿è¡Œ
python main.py

# è¾“å‡ºç›®å½•ï¼šoutputs/{exp_id}/
# â”œâ”€â”€ lora_adapter/         # LoRAæƒé‡ï¼ˆå¯ä»¥è½¬ç§»åˆ°å…¶ä»–æ¨¡å‹ï¼‰
# â”œâ”€â”€ classifiers.pt        # åˆ†ç±»å¤´æƒé‡
# â”œâ”€â”€ tokenizer/            # åˆ†è¯å™¨
# â””â”€â”€ logs/                 # è®­ç»ƒæ—¥å¿—
```

### 5. æ¨¡å‹è¯„ä¼°

```bash
python evaluator.py --config configs/my_experiment.yaml
```

### 6. æ¨ç†é¢„æµ‹

```bash
python predict.py --config configs/my_experiment.yaml --text "å¾…é¢„æµ‹æ–‡æœ¬"
```

### 7. å¯åŠ¨APIæœåŠ¡

```bash
# å®‰è£…FastAPIå’ŒUvicorn
pip install fastapi uvicorn

# å¯åŠ¨æœåŠ¡ï¼ˆé»˜è®¤ç«¯å£8000ï¼‰
uvicorn api.app:app --reload --host 0.0.0.0 --port 8000

```

---

## ğŸ’¡ æ ¸å¿ƒæŠ€æœ¯åŸç†

### LoRA (Low-Rank Adaptation)

LoRAé€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„æ³¨æ„åŠ›å±‚æ·»åŠ ä½ç§©é€‚é…çŸ©é˜µï¼Œæ¥é«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ï¼š

```
åŸå§‹å‚æ•°æ›´æ–°ï¼šÎ”W = BAï¼ˆç§©ä¸ºrçš„åˆ†è§£ï¼‰

ä¼˜åŠ¿ï¼š
- å‚æ•°é‡ï¼šä» dÃ—d é™ä½åˆ° (d+d)Ã—rï¼ˆr<<dï¼‰
- BERT-Base: 110Må‚æ•° â†’ ä»…éœ€è®­ç»ƒ 30K-300K å‚æ•°
- æ˜¾å­˜å ç”¨ï¼šå‡å°‘80%ä»¥ä¸Š
- è®­ç»ƒé€Ÿåº¦ï¼šå¿«3-5å€
```

### å¤šä»»åŠ¡å­¦ä¹ 

æ”¯æŒä¸¤ç§ä»»åŠ¡é…ç½®ï¼š

| ä»»åŠ¡ç±»å‹ | è¯´æ˜ | åº”ç”¨åœºæ™¯ |
|---------|------|--------|
| **single_cls** | å•ä¸ªåˆ†ç±»ä»»åŠ¡ | æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æ |
| **multi_cls** | å¤šä¸ªç‹¬ç«‹åˆ†ç±»ä»»åŠ¡ | å¤šæ ‡ç­¾åˆ†ç±»ã€å¤šä»»åŠ¡å­¦ä¹  |

---

## ğŸ“Š é¡¹ç›®äº®ç‚¹

### 1. **å‚æ•°é«˜æ•ˆ**
- LoRAå¾®è°ƒä»…éœ€0.1%-1%çš„å‚æ•°é‡
- BERT-Largeå¾®è°ƒä»…éœ€ 0.3M å‚æ•°vså®Œæ•´å¾®è°ƒ110M

### 2. **å¿«é€Ÿè¿­ä»£**
- çµæ´»çš„YAMLé…ç½®ç³»ç»Ÿ
- æ”¯æŒé…ç½®ç»§æ‰¿å’Œå¿«é€Ÿå®éªŒå¯¹æ¯”
- è‡ªåŠ¨æ—¥å¿—è®°å½•å’Œç»“æœè·Ÿè¸ª

### 3. **ç”Ÿäº§å°±ç»ª**
- å®Œæ•´çš„APIæœåŠ¡æ¥å£
- æ¨¡å‹çƒ­åŠ è½½å’Œç‰ˆæœ¬ç®¡ç†
- è¯¦ç»†çš„æ—¥å¿—å’Œé”™è¯¯å¤„ç†

### 4. **æ˜“äºä½¿ç”¨**
- æ¨¡å—åŒ–çš„ä»£ç è®¾è®¡
- æ¸…æ™°çš„æ–‡æ¡£å’Œç¤ºä¾‹
- æ”¯æŒCPU/GPUè‡ªåŠ¨æ£€æµ‹

---

## ğŸ”§ å¼€å‘è€…æŒ‡å—

### æ‰©å±•è‡ªå®šä¹‰åˆ†ç±»å™¨

```python
# ä¿®æ”¹ src/model/model_factory.py ä¸­çš„ TaskTextClassifier
class TaskTextClassifier(nn.Module):
    def _build_classifiers(self):
        # è‡ªå®šä¹‰åˆ†ç±»å¤´
        hidden_size = self.bert.config.hidden_size
        return MyCustomClassifier(hidden_size, num_classes)
```

### æ·»åŠ æ–°çš„ä¼˜åŒ–å™¨

åœ¨ `src/trainer/train_engine.py` ä¸­ä¿®æ”¹ `_build_optimizer()` æ–¹æ³•ã€‚

### è‡ªå®šä¹‰æ•°æ®é¢„å¤„ç†

ç¼–è¾‘ `src/data/data_processor.py` ä¸­çš„ `load_dataset()` å‡½æ•°ã€‚

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ ‡

| æ–¹æ¡ˆ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | æ˜¾å­˜å ç”¨ | æ•ˆæœ |
|------|--------|---------|--------|------|
| å…¨é‡å¾®è°ƒ | 110M | 12å°æ—¶ | 24GB | 92.5% |
| **LoRA (r=8)** | **0.3M** | **3å°æ—¶** | **4GB** | **92.1%** âœ¨ |
| ä¸­å±‚å†»ç»“ | 50M | 6å°æ—¶ | 12GB | 91.8% |

*æ•°æ®ä¸ºåœ¨BERT-Base Chineseä¸Šçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å®æµ‹ç»“æœ*

---

## ğŸ“ é…ç½®è¯¦è§£

### æ¨¡å‹é…ç½® (model section)

```yaml
model:
  arch: "bert-base-chinese"              # é¢„è®­ç»ƒæ¨¡å‹æ¶æ„
  path: "./models/bert-base-chinese"     # æœ¬åœ°æ¨¡å‹è·¯å¾„
  dropout: 0.15                          # Embedding dropout
  freeze_bert: False                     # æ˜¯å¦å†»ç»“BERTå‚æ•°
  
  lora:
    enabled: True                        # å¯ç”¨LoRA
    rank: 8                              # ç§©å¤§å°ï¼ˆè¶Šå°è¶Šå¿«ï¼Œæ•ˆæœè¶Šå¼±ï¼‰
    alpha: 16                            # ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸=2*rankï¼‰
    dropout: 0.05                        # LoRAå±‚dropout
    target_modules:                      # ç›®æ ‡æ¨¡å—
      - "query"
      - "key" 
      - "value"
    bias: "none"                         # åç½®é¡¹å¤„ç†æ–¹å¼
```

### è®­ç»ƒé…ç½® (train section)

```yaml
train:
  num_epochs: 14                         # è®­ç»ƒè½®æ•°
  batch_size: 20                         # æ‰¹æ¬¡å¤§å°
  gradient_accumulation_steps: 3         # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
  warmup_ratio: 0.1                      # å­¦ä¹ ç‡é¢„çƒ­æ¯”ä¾‹
  
  optimizer:
    type: "AdamW"
    groups:
      bert: 2e-6         # BERTå±‚å­¦ä¹ ç‡ï¼ˆæä½ï¼Œä¿ç•™çŸ¥è¯†ï¼‰
      lora: 1.5e-4       # LoRAå­¦ä¹ ç‡ï¼ˆä¸­ç­‰ï¼Œå¿«é€Ÿé€‚é…ï¼‰
      classifier: 5e-4   # åˆ†ç±»å¤´å­¦ä¹ ç‡ï¼ˆè¾ƒé«˜ï¼Œå¿«é€Ÿå­¦ä¹ ï¼‰
```

---

## ğŸ“„ License

è¯¥é¡¹ç›®é‡‡ç”¨ **GNU General Public License v2.0** å¼€æºåè®®ã€‚

---

## ğŸ‘¨â€ğŸ’» ä½œè€…

**balance-joe**

- GitHub: https://github.com/balance-joe
- é¡¹ç›®ä»“åº“: https://github.com/balance-joe/AtomLoRA

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

---

## â“ FAQ

**Q: ä¸ºä»€ä¹ˆé€‰æ‹©LoRAè€Œä¸æ˜¯å…¨é‡å¾®è°ƒï¼Ÿ**  
A: LoRAå¯ä»¥åœ¨99%é™ä½å‚æ•°é‡çš„æƒ…å†µä¸‹ä¿æŒæ¥è¿‘çš„æ•ˆæœï¼Œç‰¹åˆ«é€‚åˆåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸‹å¿«é€Ÿè¿­ä»£ã€‚

**Q: èƒ½å¦ç”¨äºç”Ÿäº§ç¯å¢ƒï¼Ÿ**  
A: å®Œå…¨å¯ä»¥ã€‚é¡¹ç›®æä¾›äº†FastAPI APIæœåŠ¡ï¼Œæ”¯æŒæ¨¡å‹çƒ­åŠ è½½å’Œå¹¶å‘æ¨ç†ã€‚

**Q: æ”¯æŒå“ªäº›é¢„è®­ç»ƒæ¨¡å‹ï¼Ÿ**  
A: æ”¯æŒHuggingFace Hubä¸Šçš„ä»»ä½•Transformeræ¨¡å‹ï¼ˆBERTã€ERNIEã€RoBERTaç­‰ï¼‰ã€‚

**Q: å¦‚ä½•è¿ç§»LoRAæƒé‡åˆ°å…¶ä»–æ¨¡å‹ï¼Ÿ**  
A: LoRAæƒé‡æ˜¯é€šç”¨çš„ï¼å¯ä»¥å°† `lora_adapter/` ç›®å½•ç›´æ¥åŠ è½½åˆ°ç›¸åŒæ¶æ„çš„å…¶ä»–æ¨¡å‹ã€‚

---

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤Issueæˆ–è”ç³»å¼€å‘è€…ã€‚

**æœ€åæ›´æ–°**: 2026å¹´2æœˆ
```

---

å¸Œæœ›è¿™ä»½READMEèƒ½å¸®åŠ©ä½ æ›´å¥½åœ°ä»‹ç»è¿™ä¸ªé¡¹ç›®ï¼âœ¨
